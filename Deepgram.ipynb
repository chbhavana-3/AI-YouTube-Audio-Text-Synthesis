{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube moviepy gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FxIn-QSEl5T",
        "outputId": "092aebcd-567b-423b-d6e0-f66a102a8e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: gtts in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# Function to download YouTube video\n",
        "def download_video(url, save_path):\n",
        "    yt = YouTube(url)\n",
        "    yt.streams.first().download(output_path=save_path)\n",
        "\n",
        "# Define YouTube video URL and save path\n",
        "video_url = \"https://www.youtube.com/watch?v=T-D1OfcDW1M&list=PPSV\"\n",
        "save_path = \"/content\"\n",
        "\n",
        "# Download the video\n",
        "download_video(video_url, save_path)\n"
      ],
      "metadata": {
        "id": "uRGzENnlEnz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from pytube import YouTube\n",
        "from moviepy.editor import *\n",
        "from gtts import gTTS\n",
        "# Function to extract audio from video\n",
        "def extract_audio(video_path, save_path):\n",
        "    video = VideoFileClip(video_path)\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(save_path)\n",
        "\n",
        "# Define video file path and save path\n",
        "video_path = \"/content/What is Retrieval-Augmented Generation (RAG).mp4\"\n",
        "audio_save_path = \"/content/audio.wav\"\n",
        "\n",
        "# Extract audio from video\n",
        "extract_audio(video_path, audio_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9sK39jJHgFG",
        "outputId": "39e3f611-c097-4875-a2d0-97b08b706faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_text(audio_path):\n",
        "    url = \"https://api.deepgram.com/v1/projects/40c7208e-990a-4783-883a-a29cd74b1c46/keys\"\n",
        "    audio_file = open(audio_path,'rb')\n",
        "    headers = {\n",
        "        'Authorization': 'Token 2a916ec30797a4c3e9eb49584cd31343d1b0733f',\n",
        "        'Content-Type': 'application/json'}\n",
        "    response = requests.post(url, headers=headers, data=audio_file)\n",
        "    response_json = response.json()\n",
        "    print(response_json)\n",
        "    return response_json['results'][0]['alternatives'][0]['transcript']\n"
      ],
      "metadata": {
        "id": "5HiHk4xThYQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepgram import Deepgram\n",
        "import json\n",
        "\n",
        "# Initialize Deepgram client with your API key\n",
        "dg_key = '2a916ec30797a4c3e9eb49584cd31343d1b0733f'\n",
        "dg = Deepgram(dg_key)\n",
        "\n",
        "# Specify the MIME type of the audio file\n",
        "MIMETYPE = 'wav'\n",
        "\n",
        "# Specify the path to the audio file\n",
        "AUDIO_FILE_PATH = '/content/audio.wav'\n",
        "\n",
        "# Define transcription parameters\n",
        "params = {\n",
        "    \"punctuate\": True,\n",
        "    \"model\": 'general',\n",
        "    \"tier\": 'nova'\n",
        "}\n",
        "\n",
        "# Function to transcribe audio file\n",
        "def transcribe_audio(audio_file_path, params):\n",
        "    with open(audio_file_path, \"rb\") as f:\n",
        "        source = {\"buffer\": f, \"mimetype\": 'audio/' + MIMETYPE}\n",
        "        return dg.transcription.sync_prerecorded(source, params)\n",
        "\n",
        "# Perform transcription\n",
        "try:\n",
        "    response = transcribe_audio(AUDIO_FILE_PATH, params)\n",
        "    with open(\"./transcript.json\", \"w\") as transcript:\n",
        "        json.dump(response, transcript)\n",
        "except Exception as e:\n",
        "    print(f\"Transcription failed: {e}\")\n"
      ],
      "metadata": {
        "id": "MOz9ws9FU66P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT = '/content/transcript.json'\n",
        "# The JSON is loaded with information, but if you just want to read the\n",
        "# transcript, run the code below!\n",
        "def print_transcript(transcription_file):\n",
        "  with open(transcription_file, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "        result = data['results']['channels'][0]['alternatives'][0]['transcript']\n",
        "        result = result.split('.')\n",
        "        for sentence in result:\n",
        "          print(sentence + '.')\n",
        "\n",
        "transcript=print_transcript(OUTPUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K6jdl-QVVsY",
        "outputId": "50c2a6e4-a1bd-4cde-ccab-658252c7926f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large language models.\n",
            " They are everywhere.\n",
            " They get some things amazingly right and other things very interestingly wrong.\n",
            " My name is Marina Denilevsky.\n",
            " I am a senior research scientist here at IBM Research, and I wanna tell you about a framework to help large language models be more accurate and more up to date.\n",
            " Retrieval augmented generation or rag.\n",
            " Let's just talk about the generation part for a minute.\n",
            " So forget the retrieval augmented.\n",
            " So the generation, this refers to large language models or LLMs that generate text in response to a user query referred to as a prompt.\n",
            " These models can have some undesirable behavior.\n",
            " I wanna tell you an anecdote, to illustrate this So my kids, they recently asked me this question in our solar system.\n",
            " What planet has the most moves? And my response was, oh, that's really great that you're asking this question.\n",
            " I loved space when I was your age.\n",
            " Of course, that was, like, thirty years ago.\n",
            " But I know this.\n",
            " I read an article, and the article said that it was Jupiter and ADA Moose.\n",
            " So that's the answer.\n",
            " Now Actually, there's a couple of things wrong with my answer.\n",
            " First of all, I have no source to support what I'm saying.\n",
            " Even though I confidently said, I read an article, I know the answer.\n",
            " I'm not sourcing it.\n",
            " I'm giving the answer off the top of my head.\n",
            " And also I actually haven't kept up with this for a while, and my answer is out of date.\n",
            " So we have two problems here.\n",
            " One is no source.\n",
            " And the second problem is that I am out of date.\n",
            " And these, in fact, are two behaviors that are often observed as problematic when interacting with large language models.\n",
            " They are LLM challenges.\n",
            " Now What would have happened if I'd taken a beat and first gone and looked up the answer on a reputable source, like NASA? Well, then I would have been able to say, ah, okay.\n",
            " So the answer is Saturn with a hundred and forty six moons.\n",
            " And in fact, This keeps changing because scientists keep on discovering more and more moods.\n",
            " So I have now grounded my answer in something more believable.\n",
            " I have not hallucinated or made up an answer.\n",
            " Oh, and by the way, I didn't leak personal information about how long ago it's been since I was obsessed with space.\n",
            " Alright.\n",
            " So what does this have to do with large language models? Well, How would a large language model have answered this question? So let's say that I have a user asking this question about moons.\n",
            " A large language model would constantly say.\n",
            " Okay.\n",
            " I have been trained.\n",
            " And from what I know in my parameters during my training, the answer is Jupiter.\n",
            " The answer is wrong, but You know? We don't know.\n",
            " The large language model is very confident in what it answered.\n",
            " Now what happens when you add this retriever retriever retrieval augmented part here? What does that mean? That means that now instead of just relying on what the l l m knows, we are adding a content store.\n",
            " This could be open like the internet, This can be closed, like some collection of documents, collection of policies, whatever.\n",
            " The point though now is that the LLM first goes and talks.\n",
            " To the content store and says, hey, can you retrieve from me information that is relevant to what the user's query was.\n",
            " And now with this retriever augmented.\n",
            " Answer, it's not Jupyter anymore.\n",
            " We know that it is sadder.\n",
            " What does this look like? Well, First, user prompts the LLM with their question.\n",
            " They say this is what my question was.\n",
            " And originally, if we're just talking to a generative model, The driver of model says, oh, okay.\n",
            " I know the response.\n",
            " Here it is.\n",
            " Here's my response.\n",
            " But now in the RAM framework, The general model actually has an instruction that says, no.\n",
            " No.\n",
            " No.\n",
            " First, go and retrieve relevant content.\n",
            " Combine that with the user's question and only then generate the answer.\n",
            " So the prompt now has three parts.\n",
            " The instruction to pay attention to the retrieved content, together with the user's question, Now give a response, and in fact, now you can give evidence for why your response was, what it was.\n",
            " So now hopefully you can see how does Rad help the two all on challenges that I have mentioned before.\n",
            " So first of all, I'll start with the out of date part.\n",
            " Now instead of, having to retrain your model, if new information comes up like, hey, we found some more moves.\n",
            " Now it's a Jupiter again, maybe it'll be saturn again in the future.\n",
            " All you have to do is you augment your data store.\n",
            " With new information, updated information.\n",
            " So now the next time that a user comes and asks the question, we're ready.\n",
            " We just go ahead and retrieve the most up to date information.\n",
            " The second problem source.\n",
            " Well, the large language model is now being instructed to pay attention to primary source data before giving its response and in fact, now being able to give evidence.\n",
            " This makes it less likely to hallucinate or to leak data because it's less likely to rely only on information that it learned during training.\n",
            " It also allows us to get the model to, have a behavior that can be very positive, which is knowing when to say I don't know.\n",
            " If the user's question cannot be reliably answered based on your data store, The model should say I don't know instead of making up something that is believable and may mislead the user.\n",
            " This can have a negative act as well, though, because if the retriever is not sufficiently good to give the large language model the best, most highest quality grounding information, then maybe the user's query that is answerable doesn't get an answer.\n",
            " So this is actually why lots of folks, including many of us here at IBM are working the problem on both sides.\n",
            " We are both working to improve the retriever to give the large language model the best quality, data on which to ground its response, and also the generative part so that the LLM can give the richest best response finally to the user when it generates the answer.\n",
            " Thank you for learning more about RAB and like and subscribe to channel.\n",
            " Thank you.\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "import json\n",
        "\n",
        "def print_transcript(transcription_file):\n",
        "    with open(transcription_file, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "        result = data['results']['channels'][0]['alternatives'][0]['transcript']\n",
        "        return result\n",
        "\n",
        "def text_to_audio(text, save_path):\n",
        "    tts = gTTS(text=text, lang='en')  # Adjust the language code if needed\n",
        "    tts.save(save_path)\n",
        "\n",
        "# Path to the JSON file containing the transcript\n",
        "transcription_file = '/content/transcript.json'\n",
        "\n",
        "# Extract transcript from JSON\n",
        "transcript = print_transcript(transcription_file)\n",
        "\n",
        "# Generate audio from text and save to specified path\n",
        "audio_output_path = \"/content/output1_audio.mp3\"  # Change this path if needed\n",
        "text_to_audio(transcript, audio_output_path)\n",
        "\n",
        "# Output audio file path\n",
        "print(\"Output audio file:\", audio_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8GeUOmTaJS7",
        "outputId": "3311f71c-be05-403a-c50b-ac65716dba33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output audio file: /content/output1_audio.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mUGFnu7caKUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmjXDFHOTP8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}